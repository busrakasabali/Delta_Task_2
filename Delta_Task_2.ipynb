{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Delta Task 2 Project WIP"
      ],
      "metadata": {
        "id": "eUt6AkpkCr_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wordninja"
      ],
      "metadata": {
        "id": "UgONqirGXv3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import wordninja"
      ],
      "metadata": {
        "id": "rc1cx8m1CoG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reading train and test sets into data frame\n",
        "\n",
        "btc_tweets_train = pd.read_parquet('btc_tweets_train.parquet.gzip')\n",
        "btc_tweets_test = pd.read_parquet('btc_tweets_test.parquet.gzip')"
      ],
      "metadata": {
        "id": "BYxQ757cElnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploratory Analysis to Understand the Data"
      ],
      "metadata": {
        "id": "BOhT4DLV1OYy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First start with the exploratory data analysis to understand the data structure. Both training and test data has 5 columns and 1500 observations in train and 500 in test set."
      ],
      "metadata": {
        "id": "OOUAMAQFKyZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "btc_tweets_train.info()\n",
        "btc_tweets_test.info()"
      ],
      "metadata": {
        "id": "6AtiYey3EA-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Content column has the content for sentiment analysis. The tweets include hashtags, emojis and line break elements, which may require handling before we start on tokenizing our data. And it is a mix of uppercase and lower case values"
      ],
      "metadata": {
        "id": "6E28ujpgLbIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "btc_tweets_train['content'][1641861088483368964]"
      ],
      "metadata": {
        "id": "xPrRrLBGEpfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data frame has an index column and the indexes for observations are the tweet_ID"
      ],
      "metadata": {
        "id": "jphRzo-eMBmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "btc_tweets_train.head(10)"
      ],
      "metadata": {
        "id": "aHDT9RZ8EZ7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sentiment of the content is already a boolean so dont need to change that. The train set consists heavily of positive sentiment score"
      ],
      "metadata": {
        "id": "vXXP9L_FO7ps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "btc_tweets_train.describe()"
      ],
      "metadata": {
        "id": "qEO1paSGEcuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing Function"
      ],
      "metadata": {
        "id": "p1P6exn51AQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "id": "IU2z9MmNmK0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a POS Tag (Part of Speech tagging) to use in lemmatization\n",
        "\n",
        "def get_wordnet_pos(word:str)->str:\n",
        "    \"\"\"Map POS tag to first character for lemmatization\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pos: str\n",
        "        The positional tag of speech retrieved from wordnet database.\n",
        "    \"\"\"\n",
        "\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    pos=tag_dict.get(tag,wordnet.NOUN)\n",
        "\n",
        "    return pos"
      ],
      "metadata": {
        "id": "gXeEhelxfqN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NLP_preprocessing_pipeline(sample:str)->str:\n",
        "    '''\n",
        "    Implements a NLP preprocessing pipeline specific for tweets from twitter.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    sample:str\n",
        "        The input text that requires preprocessing\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    preprocessed_textual_sample:str\n",
        "        The cleaned version of the tweet as a string after preprocessing steps have been applied.\n",
        "\n",
        "    '''\n",
        "\n",
        "    #Preprocessing steps:\n",
        "\n",
        "    #Removing of URLs:\n",
        "    preprocessed_textual_sample = re.sub(\"http\\S+\",\" \",sample)\n",
        "\n",
        "    #Changing all tokens to lower case\n",
        "    preprocessed_textual_sample = preprocessed_textual_sample.lower()\n",
        "\n",
        "    #Removing the line breaks:\n",
        "    preprocessed_textual_sample = preprocessed_textual_sample.replace('\\n',\" \")\n",
        "\n",
        "    #Removing hashtags while replacing the rest of the hastag with the splitted version of the words in hashtag\n",
        "    hashtags = re.findall(r'#\\w+', preprocessed_textual_sample)\n",
        "    for tag in hashtags:\n",
        "      words_in_hashtag = wordninja.split(tag[1:])\n",
        "      preprocessed_textual_sample = preprocessed_textual_sample.replace(tag, ' '.join(words_in_hashtag))\n",
        "\n",
        "    #Removing of non-alphabetic characters except numbers:\n",
        "    preprocessed_textual_sample = re.sub(r\"[^a-zA-Z0-9,.]\", \" \", preprocessed_textual_sample) #keeping ',' and '.' to not split numbers like 100,000\n",
        "\n",
        "    #Tokenization:\n",
        "    preprocessed_textual_sample = nltk.word_tokenize(preprocessed_textual_sample)\n",
        "\n",
        "    #Stopwords removal:\n",
        "    words_without_stopwords = []\n",
        "\n",
        "    for w in preprocessed_textual_sample:\n",
        "      if w not in stopwords.words(\"english\"):\n",
        "        words_without_stopwords.append(w)\n",
        "\n",
        "    #Now removing '.' and ',' from tokens\n",
        "    words_without_stopwords = [words for words in words_without_stopwords if words != '.']\n",
        "    words_without_stopwords = [words for words in words_without_stopwords if words != ',']\n",
        "\n",
        "    preprocessed_textual_sample = words_without_stopwords\n",
        "\n",
        "    # Lemmatize with POS Tag (Parts of Speech tagging)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    preprocessed_textual_sample = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in preprocessed_textual_sample]\n",
        "\n",
        "    #Join the tokens to create a cleaned version of strings to use in Keras Tokenizer\n",
        "    preprocessed_textual_sample = ' '.join(preprocessed_textual_sample)\n",
        "\n",
        "    return preprocessed_textual_sample\n"
      ],
      "metadata": {
        "id": "Ol2ZoIFQhuNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating the preprocessed train and test sets with the preprocessing function\n",
        "x_tweets_clean_train =  btc_tweets_train['content'].apply(NLP_preprocessing_pipeline)\n",
        "x_tweets_clean_test =  btc_tweets_test['content'].apply(NLP_preprocessing_pipeline)\n",
        "\n",
        "#Creating a df for the sentiment labels of train and test sets\n",
        "y_tweets_train = btc_tweets_train['sentiment']\n",
        "y_tweets_test = btc_tweets_test['sentiment']"
      ],
      "metadata": {
        "id": "lLIZO2D52FW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_tweets_clean_test"
      ],
      "metadata": {
        "id": "TwnBaQxRGb6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dictionary Based Sentiment Analysis"
      ],
      "metadata": {
        "id": "KBnRA0EgHgEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pysentiment2"
      ],
      "metadata": {
        "id": "toV7Com40mLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pysentiment2 as ps\n",
        "dictionary = ps.HIV4()\n",
        "\n",
        "#Creating a function for sentiment score assesment using a dictionary\n",
        "def get_sentiment_score(text):\n",
        "  score = round(dictionary.get_score(dictionary.tokenize(text))['Polarity'], 2)\n",
        "  return score\n",
        "\n",
        "#Assesing polarity scores using sentiment dictionary\n",
        "dc_test_sentiment_scores = x_tweets_clean_test.apply(get_sentiment_score)\n"
      ],
      "metadata": {
        "id": "ovmgxuJU7U-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dc_test_sentiment_scores.hist()"
      ],
      "metadata": {
        "id": "XplJMzbs-4Gy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function for Assesing Sentiment Classifier"
      ],
      "metadata": {
        "id": "6KhMj3gUPTrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, roc_curve\n",
        "\n",
        "def assess_sentiment_classifier(y_test, y_pred_prob, cut_off=0.5, plot_roc=True):\n",
        "    \"\"\"\n",
        "        Function to assess the classification results from the model.\n",
        "        Calculates accuracy score, roc auc score and confusion matrix.\n",
        "        Plots a roc curve when true.\n",
        "\n",
        "    \"\"\"\n",
        "    # Calculate discrete class predictions\n",
        "    y_pred_discrete = np.where(y_pred_prob>cut_off, 1, 0)\n",
        "\n",
        "    # Calculate classification accuracy and AUC\n",
        "    acc = round(accuracy_score(y_test, y_pred_discrete),4) #accurately predicted ones: TP+TN/all sample\n",
        "    auc = round(roc_auc_score(y_test, y_pred_prob),4) #TP/FP\n",
        "\n",
        "    # Confusion matrix\n",
        "    cmat = confusion_matrix(y_test, y_pred_discrete)\n",
        "\n",
        "    # ROC analysis\n",
        "    if plot_roc==True:\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
        "        plt.plot(fpr,tpr, label=\"AUC={:.4}\".format(auc));\n",
        "        plt.plot([0, 1], [0, 1], \"r--\")\n",
        "        plt.ylabel('True positive rate')\n",
        "        plt.xlabel('False positive rate')\n",
        "        plt.legend(loc='lower right')\n",
        "        plt.show();\n",
        "\n",
        "    return(auc, acc, cmat)"
      ],
      "metadata": {
        "id": "1iA1xfzRPcEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Assessing Dictionary Based Sentiment Classifier"
      ],
      "metadata": {
        "id": "jAs9iiblUd_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We make the cut_off=0 because the dictionary returns score in range (-1,1) where score<0 is negative while score>0 is positive\n",
        "assess_sentiment_classifier(y_tweets_test, dc_test_sentiment_scores, cut_off = 0,plot_roc=True)"
      ],
      "metadata": {
        "id": "xTJN-teARYDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auc_dict, acc_dict, _ = assess_sentiment_classifier(y_tweets_test, dc_test_sentiment_scores, cut_off = 0,plot_roc=True)"
      ],
      "metadata": {
        "id": "sACiw7mh6aj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating an empty data frame for results\n",
        "df_scores = pd.DataFrame(index=['ACC', 'AUC'])\n",
        "df_scores['Dictionary'] = [acc_dict,auc_dict]"
      ],
      "metadata": {
        "id": "DfYa3kYg6-Zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Document Level Sentiment Analysis Using TF-IDF Vectorizer"
      ],
      "metadata": {
        "id": "QoggdNxcYmCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "def dummy_fun(content):\n",
        "    return content\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    analyzer = 'word',\n",
        "    tokenizer = nltk.word_tokenize,\n",
        "    preprocessor = dummy_fun,\n",
        "    token_pattern = None)\n",
        "\n",
        "#Feature Extraction Using TFIDF\n",
        "x_tweets_clean_train_tfidf = tfidf_vectorizer.fit_transform(x_tweets_clean_train) #fit training data to create the vocabulary and transform\n",
        "x_tweets_clean_test_tfidf = tfidf_vectorizer.transform(x_tweets_clean_test) #transform test data\n",
        "\n",
        "#Fitting the training data into a logistic regression function\n",
        "logit = LogisticRegression()\n",
        "logit.fit(x_tweets_clean_train_tfidf,y_tweets_train)\n",
        "tfidf_test_prob = logit.predict_proba(x_tweets_clean_test_tfidf)[:,1] #Probability of being positive\n",
        "tfidf_test_discrete = np.where(tfidf_test_prob>0.5, 1, 0)"
      ],
      "metadata": {
        "id": "2jR4zW4uY0vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assess_sentiment_classifier(y_test=y_tweets_test,\n",
        "                            y_pred_prob = tfidf_test_prob)"
      ],
      "metadata": {
        "id": "JbFminPtcAY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auc_tfidf, acc_tfidf, _ = assess_sentiment_classifier(y_test=y_tweets_test,y_pred_prob = tfidf_test_prob)"
      ],
      "metadata": {
        "id": "rkGNZxTsf7ZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_scores['TFIDF'] = [acc_tfidf,auc_tfidf]\n",
        "df_scores"
      ],
      "metadata": {
        "id": "eh3Oi0ch7ESO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Processing Train & Test Sets to use in RNN"
      ],
      "metadata": {
        "id": "AYxFGb84JvIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Building vocabulary using Keras Tokenizer:\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "vocab_word_number = 6000\n",
        "\n",
        "# Create tokenizer object\n",
        "tok = Tokenizer(vocab_word_number, oov_token=1)\n",
        "\n",
        "# We fit the tokenizer to build vocabulary from the training set tweets\n",
        "tok.fit_on_texts(x_tweets_clean_train)"
      ],
      "metadata": {
        "id": "kP0qlRqCJ_f2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Number of unique words in the vocab\n",
        "len(tok.word_counts)"
      ],
      "metadata": {
        "id": "a6ahAbJlNcFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert training set tweets to sequences of integer values\n",
        "x_tweets_clean_train_int = tok.texts_to_sequences(x_tweets_clean_train)\n",
        "\n",
        "# Determine the maximum review length in the training set\n",
        "max_review_length = max([len(review) for review in x_tweets_clean_train_int])\n",
        "print('The longest tweet of the training set has {} words.'.format(max_review_length))"
      ],
      "metadata": {
        "id": "aH8QkdfRNp3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Padding to create equal length tweets to ensure a consistent sequence length\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "x_tweets_train_pad = pad_sequences(x_tweets_clean_train_int, max_review_length)"
      ],
      "metadata": {
        "id": "RglQZpfrTVqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding and padding the test data set\n",
        "x_tweets_clean_test_int = tok.texts_to_sequences(x_tweets_clean_test)\n",
        "x_tweets_test_pad = pad_sequences(x_tweets_clean_test_int, max_review_length)"
      ],
      "metadata": {
        "id": "WlcMF4k3cUE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN Language Classifier - Using Own Embeddings"
      ],
      "metadata": {
        "id": "g1eK86mPbcQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, GRU, Flatten, Dropout, LSTM, Bidirectional\n",
        "from keras.initializers import Constant\n",
        "\n",
        "# Create an embedding layer\n",
        "number_of_words = vocab_word_number\n",
        "embedding_dim = 30\n",
        "emb_layer = Embedding(input_dim = number_of_words,\n",
        "                      output_dim= embedding_dim,\n",
        "                      input_length= max_review_length)"
      ],
      "metadata": {
        "id": "YP9fwHOoc8X6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bidirectional GRU text classifier\n",
        "number_hidden_nodes = 20    # number of hidden nodes\n",
        "\n",
        "gru = Sequential()\n",
        "gru.add(emb_layer)\n",
        "gru.add(GRU(number_hidden_nodes))\n",
        "gru.add(Dropout(0.2))\n",
        "gru.add(Dense(1, activation = 'sigmoid'))\n",
        "\n",
        "gru.compile(optimizer='adam', loss = 'binary_crossentropy', metrics=['accuracy'])\n",
        "print(gru.summary())"
      ],
      "metadata": {
        "id": "YvrRttNDfDAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fitting the model\n",
        "nr_epoch = 10\n",
        "batch_sz = 64\n",
        "val_split = 0.25\n",
        "story = gru.fit(x_tweets_train_pad, y_tweets_train, batch_size= batch_sz, epochs= nr_epoch, validation_split=val_split)"
      ],
      "metadata": {
        "id": "JhNjmWG_gcxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yhat_gru = gru.predict(x_tweets_test_pad)"
      ],
      "metadata": {
        "id": "7QMNI6kHo5mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auc_gru_own, acc_gru_own, _ = assess_sentiment_classifier(y_test=y_tweets_test,\n",
        "                            y_pred_prob = yhat_gru)\n"
      ],
      "metadata": {
        "id": "Cdnp4WE1pSS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_scores['GRU_own_embeddings'] = [acc_gru_own,auc_gru_own]\n",
        "df_scores"
      ],
      "metadata": {
        "id": "k-ZlVAwLvik2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GRU with Pre-trained Embeddings"
      ],
      "metadata": {
        "id": "3QSIIbfuwDcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "6dIyVjObzBd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
        "\n",
        "#imdb_index = Word2VecKeyedVectors.load_word2vec_format('w2v_imdb_full_d100_e500.model', binary=False)"
      ],
      "metadata": {
        "id": "oWE6bPIcwPTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "imdb_index = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n"
      ],
      "metadata": {
        "id": "0_fq_yC2RsYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding_matrix(tokenizer, pretrain, vocab_size, verbose=0):\n",
        "    '''\n",
        "        Helper function to construct an embedding matrix for\n",
        "        the focal corpus based on some pre-trained embeddings.\n",
        "    '''\n",
        "\n",
        "    dim = 0\n",
        "    # We will use the function with different types of embeddings. Therefore,\n",
        "    # we need a condition to determine what is the right way of determining\n",
        "    # the embedding dimension.\n",
        "    if isinstance(pretrain, Word2VecKeyedVectors):\n",
        "        dim = pretrain.vector_size\n",
        "    elif isinstance(pretrain, dict):\n",
        "        dim = next(iter(pretrain.values())).shape[0]  # get embedding of an arbitrary word\n",
        "    else:\n",
        "        raise Exception('{} is not supported'.format(type(pretrain)))\n",
        "\n",
        "\n",
        "    # Initialize embedding matrix\n",
        "    emb_mat = np.zeros((vocab_size, dim))\n",
        "\n",
        "    # There will be some words in our corpus for which we lack a pre-trained embedding.\n",
        "    # In this tutorial, we will simply use a vector of zeros for such words. We also keep\n",
        "    # track of the words to do some debugging if needed\n",
        "    oov_words = []\n",
        "    # Below we use the tokenizer object that created our task vocabulary. This is crucial to ensure\n",
        "    # that the position of a words in our embedding matrix corresponds to its index in our integer\n",
        "    # encoded input data\n",
        "    v = len(tokenizer.word_index)\n",
        "    #start = time.time()\n",
        "    #print('Start embedding process for {} words.'.format(v), flush=True)\n",
        "\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        # try-catch together with a zero-initilaized embedding matrix achieves our rough fix for oov words\n",
        "        try:\n",
        "            emb_mat[i] = pretrain[word]\n",
        "        except:\n",
        "            oov_words.append(word)\n",
        "        # Some output that the method is still alive\n",
        "        if i % 5000 == 0 and verbose>0:\n",
        "            print('{}/{} words in {} sec'.format(i, v, (time.time()-start)), flush=True)\n",
        "\n",
        "\n",
        "    #print('Created embedding matrix of shape {} in {} min '.format(emb_mat.shape, (time.time()-start)/60))\n",
        "\n",
        "    print('Encountered {} out-of-vocabulary words.'.format(len(oov_words)))\n",
        "    return (emb_mat, oov_words)"
      ],
      "metadata": {
        "id": "hcTW_hTt2h3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create embedding weight matrix\n",
        "imdb_embeddings, oov_words = get_embedding_matrix(tok, imdb_index, vocab_word_number)"
      ],
      "metadata": {
        "id": "64SBgp1L3q_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating embedding layer using the pre-trained embeddings\n",
        "pre_trained_emb_layer = Embedding(\n",
        "    input_dim = vocab_word_number,\n",
        "    output_dim = imdb_embeddings.shape[1],\n",
        "    input_length = max_review_length,\n",
        "    embeddings_initializer = Constant(imdb_embeddings),\n",
        "    trainable = False\n",
        ")"
      ],
      "metadata": {
        "id": "hIKQ0Cgj7ISE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre_trained GRU text classifier\n",
        "number_hidden_nodes = 20    # number of hidden nodes\n",
        "\n",
        "pre_trained_gru = Sequential()\n",
        "pre_trained_gru.add(pre_trained_emb_layer)\n",
        "pre_trained_gru.add(GRU(number_hidden_nodes, activation = 'relu'))\n",
        "pre_trained_gru.add(Dropout(0.2))\n",
        "pre_trained_gru.add(Dense(1, activation = 'sigmoid'))\n",
        "\n",
        "pre_trained_gru.compile(optimizer='adam', loss = 'binary_crossentropy', metrics=['accuracy'])\n",
        "print(pre_trained_gru.summary())"
      ],
      "metadata": {
        "id": "TYbg1ljg8AYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fitting the model\n",
        "nr_epoch = 10\n",
        "batch_sz = 64\n",
        "val_split = 0.25\n",
        "story = pre_trained_gru.fit(x_tweets_train_pad, y_tweets_train, batch_size= batch_sz, epochs= nr_epoch, validation_split=val_split)"
      ],
      "metadata": {
        "id": "5j3dYTSf8bXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yhat_pre_trained_gru = pre_trained_gru.predict(x_tweets_test_pad)"
      ],
      "metadata": {
        "id": "BCNkfvgM8i2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auc_pre_trained_gru, acc_pre_trained_gru, _ = assess_sentiment_classifier(y_test=y_tweets_test,\n",
        "                            y_pred_prob = yhat_pre_trained_gru)"
      ],
      "metadata": {
        "id": "M2tMjh0n8zSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_scores['Pre_Trained_GRU'] = [acc_pre_trained_gru,auc_pre_trained_gru]\n",
        "df_scores"
      ],
      "metadata": {
        "id": "e0kVKhMo88oT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentiment Analysis Using Pre-Trained Transformer Model"
      ],
      "metadata": {
        "id": "MEz68ckbZcgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\n",
        "    task=\"text-classification\",\n",
        "    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
        "    truncation = True\n",
        ")"
      ],
      "metadata": {
        "id": "wczJcCSRZn8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pipeline_classify(data):\n",
        "    ''' Function to run the sentiment analysis pipeline on each row of a dataset\n",
        "    and extract the scores. '''\n",
        "\n",
        "    scores = []\n",
        "    for row in data:\n",
        "      output = classifier(row)[0]\n",
        "      score = output['score']\n",
        "      label = output['label']\n",
        "      score = score if label == 'POSITIVE' else -score\n",
        "      scores.append(score)\n",
        "\n",
        "    return scores\n"
      ],
      "metadata": {
        "id": "8WvN2LvIefjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_tweets_clean_test.head()"
      ],
      "metadata": {
        "id": "zkayYG-HfYF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yhat_distilbert = pipeline_classify(x_tweets_clean_test)"
      ],
      "metadata": {
        "id": "uleetYJJepuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yhat_distilbert"
      ],
      "metadata": {
        "id": "_oWdYbk5hr1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(yhat_distilbert)"
      ],
      "metadata": {
        "id": "OG5WTPFRhhbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yhat_distilbert_discrete = np.where(pd.DataFrame(yhat_distilbert)>0.5, 1, 0)\n",
        "acc_distilbert = accuracy_score(y_tweets_test, yhat_distilbert_discrete)\n",
        "auc_distilbert = roc_auc_score(y_tweets_test, yhat_distilbert_discrete)"
      ],
      "metadata": {
        "id": "63Dotl9Vg8ND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yhat_distilbert_discrete = np.where(pd.DataFrame(yhat_distilbert)>0.5, 1, 0)\n",
        "auc_distilbert, acc_distilbert, _ = assess_sentiment_classifier(y_test=y_tweets_test,\n",
        "                            y_pred_prob = yhat_distilbert_discrete)"
      ],
      "metadata": {
        "id": "Nr4f2qrigVAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_scores['DistilBert'] = [acc_distilbert,auc_distilbert]\n",
        "df_scores"
      ],
      "metadata": {
        "id": "31zyCNt8gdiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-Tuning Pre-trained Transformer Model"
      ],
      "metadata": {
        "id": "hNmeEZY4ygyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importig Distilbert tokenizer\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
      ],
      "metadata": {
        "id": "Z6NqqBM708wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_tweets_train.head()"
      ],
      "metadata": {
        "id": "0LRO3yMjCp9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_tweets_clean_train.head()\n"
      ],
      "metadata": {
        "id": "mc5ssLMZBEdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize training tweetd\n",
        "x_train_tokenized_distilbert = tokenizer(x_tweets_clean_train.tolist(), truncation=True, padding='max_length')"
      ],
      "metadata": {
        "id": "Uk37hbijA6Xw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a torch dataset for training\n",
        "\n",
        "class Tweets_Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, content, sentiment):\n",
        "        self.content = content\n",
        "        self.sentiment = sentiment\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentiment)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.content.items()}\n",
        "        item['labels'] = torch.tensor(self.sentiment[idx])\n",
        "        return item\n",
        "\n",
        "train_dataset = Tweets_Dataset(x_train_tokenized_dstilbert, y_tweets_train.tolist())"
      ],
      "metadata": {
        "id": "U9aiYAt6BfPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)"
      ],
      "metadata": {
        "id": "UhaCd2nsDlpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load trainer and set arguments for training\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='รง/results',\n",
        "    num_train_epochs = 1,\n",
        "    per_device_train_batch_size=16,\n",
        "    learning_rate= 5e-5,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    seed=111,\n",
        "    data_seed=111\n",
        "\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "jjg3NHtQD2Be"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Frame for Result Comparison"
      ],
      "metadata": {
        "id": "gBKCxZul3j1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_scores"
      ],
      "metadata": {
        "id": "ZsDcQINX6BBI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}